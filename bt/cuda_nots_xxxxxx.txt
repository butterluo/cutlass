必须按threadIdx.x连续取值,否则有stride access
复习 bank conflict
https://gcc.gnu.org/onlinedocs/gcc/index.html#SEC_Contents , https://zhuanlan.zhihu.com/p/348372132 
https://github.com/NVIDIA/cutlass/blob/master/media/docs/terminology.md

调用流程:
gemm_bias_relu.cu#102#201#212->
    device/gemm.h的operator()()->
        device_kernel.h的Kernel()(这是cuda kernel)->
            kernel/gemm.h(在default_gemm.h#506初始化)的operator()()->通过'mma('调用
                mma_pipelined.h(在default_gemm.h#476初始化)的operator()()->通过'warp_mma'调用
                    mma_tensor_op_sm70.h(在default_mma.h#196#214<-default_mma_core_sm70.h#499#511初始化)的MmaVoltaTensorOp.operator()()->通过'mma('调用
                        mma_sm70.h#522(在default_mma_core_sm70.h#484#485#507初始化)的Mma.operator()()
    InstrShap貌似被各种复写

Epilogue
gemm_bias_relu->dvice/gemm->default_gemm#545
	default_epilogue_volta_tensor_op#86 OutputOp_设为LinearCombinationRelu(linear_combination_relu#79)
		epilogue.h中的Epilogue在default_gemm中赋给kernel::Gemm
		读取bias用的是epilogue\threadblock\predicated_tile_iterator.h,计算thread与元素的映射用的是epilogue\threadblock\default_thread_map_volta_tensor_op.h,但有点复杂???为何用多维张量(col,row,grp,clu,til)

数据流向:
device_kernel.h中初始化动态smem???具体用了那种数据结构<-
predicated_tile_iterator.h#333中pred_guard怎么计算
reg到smem的regular_tile_iterator_tensor_op_sm70.h#1187没有像少侠那样用汇编写入smem,但后者却又多了次smem的地址转换,哪种比较快?


多层tile迭代:(gemm_bias_relu为例)
ThrdBlk lvl
    并行形状,在device/gemm.h#443中对grd,blk形状的计算
    threadblock_swizzle.h控制了如何切分grd和blk,以达到高效使用glb和缓存
    例中grd,当split_k_slices=1且GemmIdentityThreadblockSwizzle<1>时,和get_tiled_shape结果一样(prblmSz/blkTil.M向上取整,prblmSz/blkTil.N向上取整,split_k_slices=1)
    例中blk,见kernle/gemm.h的kThreadCount=[32*(blkM/wrpM)*(blkN/wrpN)*(blkK/wrpK)]
BlkTile lvl
    并行形状在bias_relu中客制化ShapeMMAThreadBlock=GemmShape<128, 128, 32>
    每个blk中循环次数,在kernel/gemm.h#219中计算gemm_k_iterations,即pblmSz.K/BlkTil向上取整

    thrd的每次访存取kElementsPerAccess个元素
        又由kAccessesPerVector次访存组成,每次取AccessType::kElements个元素
        AccessType(AlignedArray)是thrd最小访存取数单位,一个thrd会取kAccessesPerVector次,以凑成Fragment(Array)
Wrp lvl
    并行形状在bias_relu中客制化ShapeMMAWarp=GemmShape<64, 64, 32>
Instr lvl
    并行形状在default_gemm#456中写死为GemmShape<8, 8, 4>





数据形状变化:

计算坐标变化:

advance dimension: 移动的那个rank


循环:
一个Blk中BlkTil沿K滑动:
	kernel/gemm#235 滑动次数为pblmSz.K/BlkTil.K向上取整,这里是没有unroll的,估计是担心指令太多还是编译器过度优化???
一个WrpTil包含了多次mma(kWarpGemmIterations)
	mma_base#115中有注释 kWarpGemmIterations=WrpTil.K/mma指令在K的长度(4)
一次mma_tensor_op操作的循环请参考mma_tensor_op_sm70#240的一堆注释
	除了BlkTil,WrpTil,还有固定为<32,32,4>的InterleavedTileShape,以及更小的ArchMmaOperator::Shape(即arch::Mma::Shape,固定为<16,16,8>)
	而真正执行时的arch::Mma::Shape则对应了mma.sync.aligned.m8n8k4指令,即<8,8,4>

约束:
kernel/gemm.h中
	BlkTil要被WrpTil整除M,N
mma_pipelined&mma_base:
	#137 kStages==2
default_mma_core_sm70#395
	BlkTil要被WrpTil整除M,N
mma_tensor_op_sm70中:
	WrpTil需要被InterleavedTileShape<32,32,4>整除M,N ??? InterleavedTileShape是干嘛的
	WrpTil需要被ArchMmaOperator::Shape<16,16,4>整除M,N ??? 为何是<16,16,4>
epilogue
	SharedLoadIterator::Fragment::kElements == OutputTileIterator::Fragment::kElements
	OutputTileIterator::kElementsPerAccess
	!(OutputTileIterator::Fragment::kElements % OutputTileIterator::kElementsPerAccess)
	kPartitionsK == 1 || Base::kFragmentsPerIteration == 1
default_epilogue_volta_tensor_op
	kSharedMemAlignment == 8, "Shared memory alignment must be 8B"

???
BlkTil不在K维度划分,而是在K维度迭代么
SmemIterA用到的SmemLayoutA是针对64b优化的,是否要设置smem每个bank大小为64b,Cutls有该默认设置么
关于无法整除的边界问题的各种解决方案以及padding方法